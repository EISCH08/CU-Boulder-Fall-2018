{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Intelligence vs Machine Learning\n",
    "\n",
    "\n",
    "Agents and States\n",
    "    -Agents\n",
    "        -An agent is an entitty that percieves and acts\n",
    "            -Recieves via sensors(percepts)\n",
    "            -acts via actuators(actions)\n",
    "        -Agent Function: percept sequence -> action\n",
    "            -external characterization\n",
    "        -Agent Program: archetecture to produce\n",
    "        -Agents have goals(objectives)\n",
    "        -Example:\n",
    "            -Roomba\n",
    "                -Perceives: location, clean/dirty\n",
    "                -Actions: move left, move right, vacuum, nothing\n",
    "        -How to analyze rationality?\n",
    "            -Performance measure\n",
    "            -Environment familiarity\n",
    "            -Actions\n",
    "            -Sequences of percepts(memory)\n",
    "        -Exploring\n",
    "        -Autonomy\n",
    "        -Fully observable vs partially observable vs unobservable\n",
    "            -Fully\n",
    "                -agent can observe all relevant aspects of environment\n",
    "            -Partially\n",
    "                -some parts missing from sensor data\n",
    "            -Unobservable\n",
    "                -Cannot make any observations\n",
    "        -Single vs Multi-agent\n",
    "            -Single not having influence from another agent\n",
    "            -Multi having influence\n",
    "                -Mario Kart\n",
    "        -Deterministic vs Stochastic\n",
    "            -Deterministic: the next state of the enviroment is completely determined by the current state and action \n",
    "            executed by the agent\n",
    "            -Stochastic: non-deterministic, exhibits behavior determined by some randomness\n",
    "            -Environment is uncertain if it is not fully observabel and deterministic\n",
    "        -Episodic vs Sequential\n",
    "            -Episodic: Experience broken into discrete episodes, non of which rely on another\n",
    "            -Sequential: Current decision affects future decisions\n",
    "        -Static vs Dynamic\n",
    "            -does the environment change while the angent deliberates?\n",
    "        -Agent Types\n",
    "            -Simple Reflex angents\n",
    "                -Selects an action based on the present\n",
    "                -Does not consider future consequences\n",
    "            -Model-based reflex\n",
    "                -Selects action based on the past and present\n",
    "                -Maintains an internal state to keep track of what the world looks like\n",
    "            -Goal Based Agent\n",
    "                -Selects an action based on the past, present and future towards a prespecified goal\n",
    "                -Search and planning\n",
    "            -Utility based agent\n",
    "                -As opposed to goals, quantifies the benefit of various actions\n",
    "    -States\n",
    "        -The state of all possible world configurations(\"states-of-the-world\")\n",
    "        -State sapce graph: a mathematical representation of the problem\n",
    "            -Each state has a vertex on the graph\n",
    "            -Directed edges connect states by corresponding agent actions \n",
    "            -A search problem(for games)\n",
    "                -State Space\n",
    "                    -possible ways the world could look\n",
    "                    -paths\n",
    "                    sum of step costs\n",
    "                -Transition model\n",
    "                    -fucntion that returns state_new from doing an action to state_old\n",
    "                -Actions\n",
    "                    -operations on the environment\n",
    "                -Initial state\n",
    "                    -state at the start\n",
    "                -Goal \"test\"\n",
    "                    -whether the given state is the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Decision-Making Lecture 4\n",
    "\n",
    "Decisisons are affectd by many things\n",
    "    -What we value\n",
    "    -Uncertainty\n",
    "        -Probability Theory\n",
    "        -Ontological Commitments\n",
    "            -0 or 1\n",
    "            -Propositional Logic\n",
    "        -Loss functions\n",
    "            -Expected value of including uncertainty(EVIU)\n",
    "            -Expected value of perfect information(EVPI)\n",
    "            -Bayes decision is the decision that minimizes the expected loss\n",
    "            -Airport problem\n",
    "                -d = how long you allow yourself to drive\n",
    "                -x represents how long it will actually take you to drive\n",
    "                \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
